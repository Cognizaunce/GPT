{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9724a943",
   "metadata": {},
   "source": [
    "# Poem Generation using GPT\n",
    "\n",
    "In this notebook, we will generate a simple shakespearian poem using Generative Pretrained Transformers (GPT) that we implemented. This notebook will demonstrate poem generation character by character rather than the typical word by word generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e21087",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2466baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from gpt import GPT, GPTconfig\n",
    "from gpt import TrainingConfig, Trainer\n",
    "from gpt import sample_context\n",
    "# Could have imported all of them at once. Doesn't matter :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae7a85",
   "metadata": {},
   "source": [
    "Setting Manual Seed to avoid varying results with every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b21d6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt.utils.utils import seed_all\n",
    "\n",
    "seed_all(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e79ba2",
   "metadata": {},
   "source": [
    "## Poem Dataset class \n",
    "\n",
    "We will now use the Dataset class from `torch.utils.data` to setup our own dataset class for the dataloader. This class is responsible for loading the data from disk and generating chunks of characters. The training data will be a chunk of characters where chunk is a block_size (T). The targets for training would be the same as the training data but offset by one character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "863c6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as Dataset\n",
    "\n",
    "class CharacterDataset(Dataset):\n",
    "        def __init__(self, data, block_size):\n",
    "            characters = sorted(list(set(data)))\n",
    "            data_size, vocab_size = len(data), len(characters)\n",
    "            \n",
    "            print(f\"Dataset has {data_size} characters. {vocab_size} of characters are unique.\")\n",
    "            \n",
    "            # char to idx mapping and vice-versa\n",
    "            self.stoi = {ch:i for i,ch in enumerate(characters)}\n",
    "            self.itos = {i:ch for i,ch in enumerate(characters)}\n",
    "            \n",
    "            self.block_size = block_size\n",
    "            self.vocab_size = vocab_size\n",
    "            self.data_size = data_size\n",
    "            self.data = data\n",
    "            \n",
    "        def __len__(self):\n",
    "            return self.data_size - self.block_size\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # take a chunk of data from the given index from the dataset\n",
    "            chunk = self.data[idx : idx + self.block_size + 1]\n",
    "            \n",
    "            #convert the chunk to integers\n",
    "            data = [self.stoi[ch] for ch in chunk]\n",
    "            \n",
    "            # create x and y. \n",
    "            # x will contain every but the last character in the chunk.\n",
    "            # y will contain every but the first character in the chunk.\n",
    "            # Hence this will create an offset in targets by 1.\n",
    "            # Thus helps in language modelling. Given a character, the goal of the transformer would be to predict the next character in sequence.\n",
    "            \n",
    "            x = torch.tensor(data[:-1], dtype=torch.long) # nn.Embedding requires input data to be in torch.long\n",
    "            y = torch.tensor(data[1:], dtype=torch.long)\n",
    "            \n",
    "            return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dab98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/input.txt\"\n",
    "BLOCK_SIZE = 128 # spatial context of the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5cf0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(DATA_PATH, \"r\")\n",
    "data = file.read(-1) # -1 means read the whole file. If file size is large, you may want to consider replacing it with the number of characters to be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db970d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 0 characters. 0 of characters are unique.\n"
     ]
    }
   ],
   "source": [
    "dataset = CharacterDataset(data = data, block_size = BLOCK_SIZE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8af62",
   "metadata": {},
   "source": [
    "### Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75dff3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTraining Data : \u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mTargets : \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = dataset[1] # returns a tuple of tensors at idx = 0. Feel free to chnage the idx\n",
    "x, y = batch\n",
    "x = x.tolist()\n",
    "y = y.tolist()\n",
    "\n",
    "for i in range(len(x)):\n",
    "    x[i] = dataset.itos[x[i]]\n",
    "    y[i] = dataset.itos[y[i]]\n",
    "\n",
    "print(f\"\\033[1mTraining Data : \\033[0m\\n\\n{''.join(x)}\")\n",
    "print(f\"\\n\\033[1mTargets : \\033[0m\\n\\n{''.join(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70981a",
   "metadata": {},
   "source": [
    "## Configuring GPT\n",
    "\n",
    "Now that we have finished setting up the dataset class, its now time to train our GPT model on this dataset. Before we start training, we will configure the GPT with appropriate model parameters. \n",
    "\n",
    "Because original GPT (referring to GPT3) requires huge computational resources, we will be using a smaller GPT model. This model, though being small, is by itself a very good model. A single layer model can learn to generate poems with fairly good accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b8169b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_config = GPTconfig(num_layers = 2, \n",
    "                       n_heads = 12, \n",
    "                       embd_size = 768, \n",
    "                       vocab_size = dataset.vocab_size, \n",
    "                       block_size = dataset.block_size\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf2f5550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable Parameters :  14275584\n"
     ]
    }
   ],
   "source": [
    "model = GPT(gpt_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d99847",
   "metadata": {},
   "source": [
    "## Training the GPT model\n",
    "\n",
    "GPT model has been configured. Now it is time to train it on our dataset. As mentioned earlier, training the model requires lot of computational time and resources. So the amount of time taken to train depends on the kind of system you have. The training loop is designed to work with multiple GPUs if you have access to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96b0ecaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "__len__() should return >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Julian\\OneDrive\\Workspaces\\Python for ML\\projects\\GPT\\PoemGeneration.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julian/OneDrive/Workspaces/Python%20for%20ML/projects/GPT/PoemGeneration.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_config \u001b[39m=\u001b[39m TrainingConfig(max_epochs \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julian/OneDrive/Workspaces/Python%20for%20ML/projects/GPT/PoemGeneration.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                               batch_size \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julian/OneDrive/Workspaces/Python%20for%20ML/projects/GPT/PoemGeneration.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                               lr_decay \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julian/OneDrive/Workspaces/Python%20for%20ML/projects/GPT/PoemGeneration.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                               lr \u001b[39m=\u001b[39m \u001b[39m6e-4\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julian/OneDrive/Workspaces/Python%20for%20ML/projects/GPT/PoemGeneration.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                               warmup_tokens \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\u001b[39m*\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Julian/OneDrive/Workspaces/Python%20for%20ML/projects/GPT/PoemGeneration.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                               final_tokens \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39;49m(dataset) \u001b[39m*\u001b[39m dataset\u001b[39m.\u001b[39mblock_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julian/OneDrive/Workspaces/Python%20for%20ML/projects/GPT/PoemGeneration.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                               ckpt_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./checkpoints/transformers.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Julian/OneDrive/Workspaces/Python%20for%20ML/projects/GPT/PoemGeneration.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                              )\n",
      "\u001b[1;31mValueError\u001b[0m: __len__() should return >= 0"
     ]
    }
   ],
   "source": [
    "train_config = TrainingConfig(max_epochs = 2, \n",
    "                              batch_size = 256, \n",
    "                              lr_decay = True, \n",
    "                              lr = 6e-4,\n",
    "                              warmup_tokens = 512*20,\n",
    "                              final_tokens = 2 * len(dataset) * dataset.block_size,\n",
    "                              ckpt_path = \"./checkpoints/transformers.pt\"\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c0401",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model = model, train_set = dataset, test_set = None, configs = train_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24275f11",
   "metadata": {},
   "source": [
    "## Let's generate poems\n",
    "\n",
    "The model has been trained and it would have learnt the mappings of different sequences. Now, we will seed it with a starting context and ask the model to predict the next character seq by seq until we are done sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab669e26",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "seed_context = \"Help me!\"\n",
    "x = torch.tensor([dataset.stoi[s] for s in seed_context], dtype=torch.long, device=trainer.device)[None,...]\n",
    "y = sample_context(model=model, x=x, steps=10000, temperature=1.0, sample=True, top_k=10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12530d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "y = y.tolist()\n",
    "y = [dataset.itos[i] for i in y]\n",
    "y = \"\".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d27da7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(f\"\\033[1mGenerated Data : \\033[0m\\n\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e93e98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook shows that transformers can learn to generate not just word by word but can also go one step further and generate character by character. Generating poems character by character is a hard task. The model must learn to recognise characters from scratch. Sequences of characters must be joined together to form meaning full sentences. \n",
    "\n",
    "Self attention modules in transformer architecture learn to pay different amounts of \"attention\" to different words (here characters). This helps the model to learn effectively and hence perform well in langauge modelling tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
